{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3b5114-822a-4f6d-8e87-6b3d5ba9d375",
   "metadata": {},
   "source": [
    "# LLMFAO: Large Language Model Feedback Analysis and Optimization\n",
    "\n",
    "This is a minimalistic [large language model](https://en.wikipedia.org/wiki/Large_language_model) (LLM) leaderboard that is based on human and machine feedback on pairwise responses of the models based on a carefully-selected set of 13 prompts and 59 different models.\n",
    "\n",
    "> When you see the outputs of two different systems for the specific query, you can determine the better one using a smaller instruction. I decided to give pairwise comparisons a try on the data kindly provided by the [llmonitor.com](https://llmonitor.com/) team.\n",
    ">\n",
    "> I asked carefully chosen crowd annotators to evaluate every pair to determine the winner. If both models performed similarly well or poorly, itâ€™s a tie. Five different annotators evaluated every pair according to the [instruction](https://github.com/dustalov/llmfao/blob/master/crowd-instruction.md); there were 124 annotators in total. I also asked GPT-3.5 Turbo Instruct and GPT-4 to do the same using a shorter [evaluation prompt](https://github.com/dustalov/llmfao/blob/master/gpt-instruction.txt), but I subjectively found human performance to be superior.\n",
    "\n",
    "A more detailed description of this study is available at <https://evalovernite.substack.com/p/llmfao-human-ranking>.\n",
    "\n",
    "The datasets and code are available on GitHub at <https://github.com/dustalov/llmfao> under open-source licenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b94fb25-f3e4-4e8b-80a5-e4fe998fcde7",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from plotly.graph_objects import Figure\n",
    "from gradio_client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1139ed2-f3e0-4a90-a819-e8539831e68e",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "client = Client('https://dustalov-pair2rank.hf.space/')\n",
    "\n",
    "def pair2rank(path: str, client: Client = client) -> pd.DataFrame:\n",
    "    rankings, _ = client.predict(path, 'Bradley-Terry (1952)', False, False, 0)\n",
    "    \n",
    "    with open(rankings, 'rb') as f:\n",
    "        rankings_json = json.load(f)\n",
    "    \n",
    "    df = pd.DataFrame(data=rankings_json['data'], columns=rankings_json['headers'])\n",
    "    df.set_index('item', inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476fdba8-1eda-4869-9676-edae9175d903",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "def pairwise(df: pd.DataFrame, n: int = 7) -> Figure:\n",
    "    scores = df['score'].to_numpy()\n",
    "\n",
    "    df_pairwise = pd.DataFrame(data=scores[:, np.newaxis] / (scores + scores[:, np.newaxis]),\n",
    "                           index=df.index, columns=df.index)\n",
    "\n",
    "    df = pd.concat((df.head(n), df.tail(n)))\n",
    "    df = df[~df.index.duplicated(keep='last')]\n",
    "\n",
    "    df_pairwise = df_pairwise.reindex(labels=df.index, columns=df.index, copy=False)\n",
    "\n",
    "    fig = px.imshow(df_pairwise, color_continuous_scale='RdBu', text_auto='.2f')\n",
    "    fig.update_layout(xaxis_title='Loser', yaxis_title='Winner', xaxis_side='top')\n",
    "    fig.update_traces(hovertemplate='Winner: %{y}<br>Loser: %{x}<br>Fraction of Wins: %{z}<extra></extra>')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ada31b-f266-4b94-bb27-9dcf458d44dc",
   "metadata": {},
   "source": [
    "## Human Judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae54ffb-3672-4119-a4a5-d05f86a184b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crowd = pair2rank('crowd-comparisons.csv')\n",
    "df_crowd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22d5839-e020-4d1c-9887-7390c286be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise(df_crowd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7447f099-195b-467a-938b-da6baed2a5b4",
   "metadata": {},
   "source": [
    "## Evaluation with GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ed992-5a3a-41eb-ada5-03a48c118b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt4 = pair2rank('gpt4-crowd-comparisons.csv')\n",
    "df_gpt4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bd6bac-907d-4cd1-bf50-0af3c53ebbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise(df_gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df0d9d-50b8-46c9-a366-45565568d4e7",
   "metadata": {},
   "source": [
    "## Evaluation with GPT-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4647c6c4-2a3a-461c-937c-85e8187d33f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt3 = pair2rank('gpt3-crowd-comparisons.csv')\n",
    "df_gpt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f082b0-e495-4f08-a814-876bda37c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise(df_gpt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10570b07-057b-45ef-8591-57d94ec599e7",
   "metadata": {},
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3eaa4-88dc-4b08-90b4-1dcd24d18672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranks = pd.concat((df_crowd['rank'], df_gpt4['rank'], df_gpt3['rank']), axis=1)\n",
    "df_ranks.columns = ['Humans', 'GPT-4', 'GPT-3']\n",
    "df_ranks.corr(method='spearman')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "title": "LLMFAO Leaderboard"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
